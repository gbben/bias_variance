<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-02-16">

<title>Bias-Variance decomposition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivations-for-investigating-the-phenomenon" id="toc-motivations-for-investigating-the-phenomenon" class="nav-link active" data-scroll-target="#motivations-for-investigating-the-phenomenon">Motivations for investigating the phenomenon:</a>
  <ul class="collapse">
  <li><a href="#who-ordered-that" id="toc-who-ordered-that" class="nav-link" data-scroll-target="#who-ordered-that">Who ordered that?</a>
  <ul class="collapse">
  <li><a href="#are-mlps-and-linear-regression-really-that-different" id="toc-are-mlps-and-linear-regression-really-that-different" class="nav-link" data-scroll-target="#are-mlps-and-linear-regression-really-that-different">Are MLPs and Linear Regression really that different?</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conceptualising-the-hypothesis-space" id="toc-conceptualising-the-hypothesis-space" class="nav-link" data-scroll-target="#conceptualising-the-hypothesis-space">Conceptualising the Hypothesis space</a></li>
  <li><a href="#a-view-into-the-generalisation-error" id="toc-a-view-into-the-generalisation-error" class="nav-link" data-scroll-target="#a-view-into-the-generalisation-error">A view into the generalisation error</a>
  <ul class="collapse">
  <li><a href="#in-a-nutshell" id="toc-in-a-nutshell" class="nav-link" data-scroll-target="#in-a-nutshell">In a nutshell</a></li>
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link" data-scroll-target="#problem-statement">Problem statement</a></li>
  <li><a href="#decomposing-the-error" id="toc-decomposing-the-error" class="nav-link" data-scroll-target="#decomposing-the-error">Decomposing the error</a>
  <ul class="collapse">
  <li><a href="#decomposition" id="toc-decomposition" class="nav-link" data-scroll-target="#decomposition">Decomposition</a></li>
  <li><a href="#simulations" id="toc-simulations" class="nav-link" data-scroll-target="#simulations">Simulations:</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#so-what-do-we-expect-for-the-out-of-sample-error" id="toc-so-what-do-we-expect-for-the-out-of-sample-error" class="nav-link" data-scroll-target="#so-what-do-we-expect-for-the-out-of-sample-error">So what do we expect for the out-of-sample error?</a>
  <ul class="collapse">
  <li><a href="#classical-error-curve" id="toc-classical-error-curve" class="nav-link" data-scroll-target="#classical-error-curve">Classical error curve</a></li>
  <li><a href="#what-i-told-you-is-not-not-wrong-but-not-fully-right-but-it-is-incomplete" id="toc-what-i-told-you-is-not-not-wrong-but-not-fully-right-but-it-is-incomplete" class="nav-link" data-scroll-target="#what-i-told-you-is-not-not-wrong-but-not-fully-right-but-it-is-incomplete">What I told you is not “Not wrong but not fully right”, but it is incomplete</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Bias-Variance decomposition</h1>
<p class="subtitle lead">Decomposing the out-of-sample error of a Learner.</p>
  <div class="quarto-categories">
    <div class="quarto-category">learning theory</div>
    <div class="quarto-category">ML</div>
    <div class="quarto-category">presentation</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 16, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="motivations-for-investigating-the-phenomenon" class="level1">
<h1>Motivations for investigating the phenomenon:</h1>
<section id="who-ordered-that" class="level2">
<h2 class="anchored" data-anchor-id="who-ordered-that">Who ordered that?</h2>
<p>There are a number of properties which have made our AI journey easier and we currently don’t really know how to explain. For instance, here are three things that have made our journey to successful AI an easier feat and which have made current systems, like LLMs, possible:</p>
<ul>
<li>Local minima that the optimisation process finds:
<ul>
<li>In <strong>huge</strong> dimensional space we’re still able to find a <strong>good enough minima</strong>. NB Global minima virtually impossible to find. We don’t seem to fall in a bad local minima.</li>
<li>Said differently <strong>we’re able to achieve a good minima with less computation than expected</strong></li>
</ul></li>
<li>Over-parametrisation and generalisation:
<ul>
<li>Num params &gt;&gt; data</li>
<li>Many final states of the model would be able to fit the training data perfectly but won’t actually model the true underlying function / process</li>
<li>i.e we’re able to generalise to unseen data</li>
<li>said differently <strong>we require far less information than we would expect, e.g.&nbsp;data set size multiple times larger than models</strong></li>
</ul></li>
<li>Emergent abilities
<ul>
<li>Tasks cannot be accomplished for any model until we reach a certain scale/ size. Past that abilities seem to emerge. All at once?</li>
</ul></li>
</ul>
<p><img src="emergent_abilities_ml.png" title="Title: Performance as scale of model is increased." class="img-fluid" alt="Emergent abilities of LLMs."></p>
<section id="are-mlps-and-linear-regression-really-that-different" class="level3">
<h3 class="anchored" data-anchor-id="are-mlps-and-linear-regression-really-that-different">Are MLPs and Linear Regression really that different?</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="LR_vs_MLP_graphs.png" title="Title: Graphical view of the two architectures" class="img-fluid figure-img" alt="Schematics of the architecture of the two models." width="3000">
</figure>
<p></p><figcaption class="figure-caption">Graphical structure of LR and single hidden layer MLP</figcaption><p></p>
</figure>
</div>
</section>
</section>
</section>
<section id="conceptualising-the-hypothesis-space" class="level1">
<h1>Conceptualising the Hypothesis space</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="hypothesis_set.png" title="Title: Graphical view of the two architectures" class="img-fluid figure-img" alt="Schematics of the architecture of the two models." width="3000">
</figure>
<p></p><figcaption class="figure-caption">Hypothesis set</figcaption><p></p>
</figure>
</div>
</section>
<section id="a-view-into-the-generalisation-error" class="level1 page-columns page-full">
<h1>A view into the generalisation error</h1>
<div class="page-columns page-full"><p>The concepts of Bias and Variance allow us to understand the out-of-sample error in terms of constituents that we can reason about clearly and which give us insights into the fundamental behaviour of systems which learn from data<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;This is inherently a frequentist view. If we view the process of learning a function from data as a process of obtaining an <em>estimator</em>, i.e.&nbsp;a point estimator in function space, then we’ll be able to decompose the expected out-of-sample error in terms of the <em>bias</em> and <em>variance</em> of the function estimator.</p></li></div></div>
<p>We shall be looking at the expected out-of-sample error and its decomposition into clearly interpretable constituents.</p>
<p>It’s important to note that the decomposition/analysis given here applies to a <strong>regression problem with squared error loss</strong>:</p>
<ul>
<li><p>In this case the decomposition is neat and this benefits the understanding of what is happening.</p></li>
<li><p>Something similar can be done, given some reformulation of definitions, for some other lossess (see <span class="citation" data-cites="domingos2000unified">Domingos (<a href="#ref-domingos2000unified" role="doc-biblioref">2000</a>)</span> reference).</p></li>
<li><p>Principle however is generically applicable. It stands across all instances of learning from data – in general however the decomposition is not as neat/clear.</p></li>
<li><p>Worth mentioning that this isn’t the only view into the out-of-sample error. VC theory studies the question of generalisation from a different point of view – i.e.&nbsp; in terms of complexity and capacity of the hypothesis space.</p></li>
</ul>
<p>Finally, note that we’re here using the term <strong>out-of-sample error</strong> instead of <strong>generalisation error</strong>.</p>
<p>I’ll be reserving the term <strong>generalisation error</strong> to mean the difference between the error in the training set and the error in the testing (or out-of-sample) set.</p>
<p>Thus, <strong>out-of-sample</strong> is the error on data not present in the set of data used for fitting the model.</p>
<section id="in-a-nutshell" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="in-a-nutshell">In a nutshell</h2>
<p>In a nutshell the Bias-Variance decomposition decomposes the expected out-of-sample error into:</p>
<ol type="1">
<li><p>How well the Hypothesis space, <span class="math inline">\(\mathcal{H}\)</span> can approximate the true function that we’re trying to fit, <span class="math inline">\(f\)</span> – i.e.&nbsp;related to the <em>approximation power</em> of the family of functions that the learning algorithm has the ability to search in<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p></li>
<li><p>How well, for a given training set, we can “zoom in” on a good hypothesis, <span class="math inline">\(h \in \mathcal{H}\)</span>. That is, when fitting to given data the learning algorithm will select a hypothesis from the hypothesis space. Thus this is related to the <em>generalisation power</em> of the given hypothesis.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;There is an important subtlety here; in reality the <em>accessible</em> space of hypothesis may well (and often is) smaller that the space of family of functions that the learning algorithm can chose from. The Hypothesis space is in practice determined by additional limitations such as the optimisation algorithm’s ability to navigate the hypothesis space. We will see how this does indeed have an effect: the <em>effective representational capacity</em> (the effective space of hypothesis) being smaller than the <em>potential representational capacity</em>. This can be thought of as a form of implicit regularization.</p></li></div><div class="page-columns page-full"><p>Note that 2 implies that we’re dealing with the selection of a hypothesis in the hypothesis space and this is affected both by the <strong>amount</strong> of available training data and the optimisation process utilised to fit the model, i.e.&nbsp;the navigation skills of the learning algorithm when producing a predictor which itself is affected by a number of things, for instance the initialisation of the weights<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;This isn’t the case for learning processes that are closed form, e.g.&nbsp;using the Normal equations to fit a linear regression model.</p></li></div></div>
</section>
<section id="problem-statement" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="problem-statement">Problem statement</h2>
<p>We will be considering a distribution that generates “regression” data. We denote this data-generation distribution by <span class="math inline">\(P_{g}\)</span>. We assume that we are given a <em>budget</em>, <span class="math inline">\(n\)</span>, and we sample <span class="math inline">\(n\)</span> times from <span class="math inline">\(P_{g}\)</span> to obtain a training data-set, <span class="math display">\[
D = \{(x_{1}, y_{1}), (x_{2}, y_{2}), ..., (x_{n}, y_{n})\},
\]</span> formed of i.i.d. points <span class="math display">\[
(x_i, y_i) \sim P_g (X, Y),
\]</span> with <span class="math inline">\(X \in \mathcal{X}\)</span> and <span class="math inline">\(Y \in \mathbb{R}\)</span>.</p>
<p>Further, we consider</p>
<ul>
<li>a <em>hypothesis space</em>, <span class="math inline">\(\mathcal{H}\)</span>, i.e.&nbsp;a class of <em>predictors</em> <span class="math inline">\(h: \mathcal{X} \rightarrow \mathbb{R}: x \mapsto \hat y\)</span>, and</li>
<li>a <em>learner</em>, <span class="math inline">\(\mathcal{L}\)</span>, i.e.&nbsp;a loss function and an optimisation algorithm, which applied to the data-set <span class="math inline">\(D\)</span> will produce a <em>predictor</em>. That is <span class="math display">\[
\mathcal{L}: \mathcal{D} \rightarrow \mathcal{H}: D \mapsto h_D = \mathcal{L} (D),
\]</span> where <span class="math inline">\(\mathcal{D}\)</span> is the space of <span class="math inline">\(n\)</span>-dimensional datasets.</li>
</ul>
<p>For our analysis we will consider a square loss error, where the loss for datum <span class="math inline">\((x, y)\)</span> is <span class="math display">\[
\delta_D (x, y) = \left( h_D (x) - y \right)^2
\]</span> and the <em>expected out-of-sample error</em> is given by <span class="math display">\[
\begin{align*}
\Delta_{P_g} &amp;= \mathbb{E}_{(x, y)} \left[ \delta_D (x, y) \right] \\
             &amp;= \int_{\mathcal{X} \times \mathbb{R}} \delta_D \left( x, y \right) P_g (x, y) \text{d}x \text{d} y.
\end{align*}
\]</span></p>
<p>This should be compared with the <em>in-sample error</em>, which is given by <span class="math display">\[
\Delta_{D} = \frac{1}{|D|} \sum_D \delta_D \left( x, y \right).
\]</span></p>
<p>It is crucial to note that there is a dependency on the training set, <span class="math inline">\(D\)</span>. Specifically, that this is the out-of-sample error <strong>given <span class="math inline">\(h_{D}\)</span></strong>, i.e.&nbsp; <span class="math display">\[
\mathbb{E}_{(x, y) \sim P_{g}|h_{D}}[(h_{D}(x) - y)^{2}]
=
\mathbb{E}_{(x, y) \sim P_{g}}[(h_{D}(x) - y)^{2} | h_{D}]
\]</span></p>
<p>Observe that we can easily ascertain, given <span class="math inline">\(h_{D}\)</span>, the in-sample error; we utilise <span class="math inline">\(D\)</span> to obtain the estimate empirically. However we are really interested in ascertaining the out-of-sample error, i.e.&nbsp;that on new data sampled from the data-generating distribution, i.e.&nbsp;<span class="math inline">\(D' \sim P_{g}\)</span> with <span class="math inline">\(D' \neq D\)</span>.</p>
<p>We cannot however compute such expectation as for that we’d need to sample infinitely many times and evaluate the error for each. However we could view this differently and ask what the out-of-sample error for the <strong>learning algorithm</strong>, <span class="math inline">\(\mathcal{L}\)</span>, is.</p>
<p>To reiterate: we start with a hypothesis space <span class="math inline">\(\mathcal{H}\)</span>, i.e.&nbsp;a learning algorithm (family of functions the learning algorithm can chose from). Then, given a dataset <span class="math inline">\(D\)</span>, we obtain a hypothesis <span class="math inline">\(h \in \mathcal{H}\)</span> (i.e.&nbsp;a model/predictor): <span class="math display">\[
\mathcal{L}: D \sim P_{g} \rightarrow h_{D} = \mathcal{L}(D)
\]</span></p>
<p>For instance, <span class="math inline">\(\mathcal{L}\)</span> could be an SVM, Perceptron, GLM etc.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><strong>Note:</strong> <span class="math inline">\(D\)</span> is a random variable (since a set of random variables is itself a random variable) and because <span class="math inline">\(h_{D} = \mathcal{L}(D)\)</span>, and <span class="math inline">\(\mathcal{L}\)</span> is (at best) a deterministic function of a random variable, then <span class="math inline">\(h_{D}\)</span> is also a random variable. Clearly, sampling a different <span class="math inline">\(D'\)</span> we would result in a different function <span class="math inline">\(h_{D'}\)</span>. We can therefore consider <span class="math inline">\(\mathcal{L}\)</span> a distribution over functions.</p>
</div></div><p>The benefit of this is that we can think of the expected hypothesis, <span class="math inline">\(\overline{h}\)</span>, which we can think of as the <strong>expected classifier</strong>: <span class="math display">\[
\overline{h} = \mathbb{E}_{D \sim P^{n}} [ \mathcal{L}(D)] = \int_{D} h_{D} P(D) dD
\]</span></p>
<p>To estimate the function <span class="math inline">\(\overline{h}\)</span> we sample the sets <span class="math inline">\(D\)</span>, train the model to get <span class="math inline">\(h_{D}\)</span> and then average the output predictions.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note however that:</p>
<ol type="1">
<li><p>the average is not the average of the weights of the fitted model. This would be fine for a linear model but not for other models. The average would be the average of the predictions for the infinitely many <span class="math inline">\(h_{D}\)</span>s</p></li>
<li><p>for classification the average prediction would be the mode</p></li>
<li><p>the “average” hypothesis is mostly a conceptual tool considered as the “best” possible hypothesis we can get using the hypothesis set, <span class="math inline">\(\mathcal{H}\)</span>. However we don’t actually know that <span class="math inline">\(\overline{h}\)</span> is indeed the best – being an average however we expect to be a pretty good hypothesis</p></li>
<li><p><span class="math inline">\(\overline{h}\)</span> might not be in the hypothesis set, i.e.&nbsp;<span class="math inline">\(\overline{h} \notin \mathcal{H}\)</span></p></li>
</ol>
</div>
</div>
<p>Because the hypothesis are random variables we can rid ourselves of the dependency on <span class="math inline">\(D\)</span> by marginalising based on a given budget of <span class="math inline">\(n\)</span> examples:</p>
<p><span class="math display">\[
\mathbb{E}_{D \sim P(X, Y)}[\Delta_{P_g}] = \mathbb{E}_{D \sim P(X, Y)} [ \mathbb{E}_{(x, y) \sim P_{g}}[(h_{D}(x) - y)^{2}]]
\]</span></p>
<p>That is, we are now considering the expected error of the learning algorithm, <span class="math inline">\(\mathcal{L}\)</span>, without conditioning on a specific <span class="math inline">\(h_{D}\)</span> any longer. Recall that <span class="math inline">\(\mathcal{L}\)</span> is some procedure to come up with a predictor, e.g.&nbsp;SVMs in general. To be very precise then we are actually studying the following quantity: <span class="math display">\[
\mathbb{E}_{\underset{D \sim P^{n}}{(x, y) \sim P}}[(h_{D}(x) - y)^{2} | \mathcal{L}]
\]</span></p>
<p>The process is thus as follows:</p>
<ol type="1">
<li><p>we fix <span class="math inline">\(\mathcal{L}\)</span>, i.e.&nbsp;the learning algorithm and hence the the hypothesis space <span class="math inline">\(\mathcal{H}\)</span></p></li>
<li><p>we sample <span class="math inline">\(D \sim P_{g}\)</span> with <span class="math inline">\(|D| = n\)</span>, for some budget, <span class="math inline">\(n\)</span></p></li>
<li><p>we train the model, thus picking an <span class="math inline">\(h \in \mathcal{H}\)</span> via <span class="math inline">\(h_{D} = \mathcal{L}(D)\)</span></p></li>
<li><p>sample the test points <span class="math inline">\((x, y)\)</span> and compute the error for all samples</p></li>
<li><p>repeat 2 - 4 again and again and compute the error.</p></li>
</ol>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
For this discussion assume no noise in the data such as to simplify the derivation.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>One final consideration should be the noise intrinsic in the data itself. This need not always be the case but it is informative to generalise to this situation. In the case where there is no stochasticity this term can simply be ignored (in fact it will drop out). In the case where the process is stochastic then we can see that, given <span class="math inline">\(x\)</span> there may not be a unique <span class="math inline">\(y\)</span>. We can see this explicitly by decomposing the distribution;</p>
<p><span class="math display">\[
P(X, Y) = P(Y|X)P(X)
\]</span> Thus, given <span class="math inline">\(x \sim P(X)\)</span> the value of <span class="math inline">\(y\)</span> is still uncertain.</p>
<p>In regression, for a given <span class="math inline">\(x\)</span>, the optimal decision is the expected <span class="math inline">\(y\)</span>, i.e.&nbsp; <span class="math inline">\(\overline{y}\)</span>. Compare this to the case of classification where, given <span class="math inline">\(x\)</span>, we predict with the Bayes optimal classifier (for classification this is the mode of the distribution). Thus, <span class="math display">\[
\overline{y}(x) = \mathbb{E}_{y|x}(y)
\]</span> and therefore the <em>expected label</em> is given by:</p>
<p><span class="math display">\[
\overline{y}(x) = \int y P(y | x ) dy
\]</span> i.e.&nbsp;for specific <span class="math inline">\(x\)</span> given all the possible <span class="math inline">\(y\)</span> given by <span class="math inline">\(P(Y|X)\)</span>, we want to predict the expected <span class="math inline">\(y\)</span>.</p>
<p><strong>Noise will add a term to the decomposition but it change the point I want to drive in this presentation.</strong></p>
</div>
</div>
</div>
</section>
<section id="decomposing-the-error" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="decomposing-the-error">Decomposing the error</h2>
<p>We have: <span class="math display">\[
\mathbb{E}_{\underset{D \sim P^{n}}{(x, y) \sim P}}[(h_{D}(x) - y)^{2} | \mathcal{L}]
=
\underset{D}{\int} \underset{x}{\int}\underset{y}{\int} [ h_{D}(x) - y ]^{2}
P(x, y | \mathcal{L}) P(D | \mathcal{L}) dydxdD
\]</span></p>
<p>However <span class="math inline">\(P(x, y | \mathcal{L}) = P(x, y)\)</span> and <span class="math inline">\(P(D | \mathcal{L}) = P(D)\)</span></p>
<p>The above quantity is what we are really interested in, the quantity that is of interest to us when chosing or designing an algorithm. It is able to inform us of the following: for a given data-generating distribution, <span class="math inline">\(P\)</span>, and a given budget, <span class="math inline">\(n\)</span>, how well are we going to do, i.e.&nbsp;how good is the (expected) out-of-sample error.</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>From now on I’ll drop the explicit dependencies to slim the notation</p>
</div>
</div>
<section id="decomposition" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="decomposition">Decomposition</h3>
<p>The trick to use is to utilise the expected classifier to modularise the expectation: <span class="math display">\[
\mathbb{E}_{\underset{D \sim P^{n}}{(x, y) \sim P}}[(h_{D}(x) - y)^{2} ]
=
\mathbb{E}_{\underset{D \sim P^{n}}{(x, y) \sim P}}[ (h_{D}(x) - \overline{h}(x) + \overline{h}(x) - y)^{2}]
\]</span> Therefore we can expand as: <span class="math display">\[
\mathbb{E}_{\underset{D}{(x, y)}}[(h_{D}(x) - y)^{2} ]
=
\mathbb{E}_{\underset{D}{x}}[(h_{D}(x) - \overline{h}(x)) ^ {2}]
+
\mathbb{E}_{(x, y)}[(\overline{h}(x) - y(x)) ^ {2}]
+
2\mathbb{E}_{\underset{D}{(x, y)}}[ (h_{D}(x) - \overline{h}(x))(\overline{h}(x) - y)]
\]</span></p>
<p>Looking at the final term we see that, utilising the <strong>expected hypothesis</strong>, the term drops out:</p>
<p><span class="math display">\[\begin{equation*}
  \begin{aligned}
    \mathbb{E}_{\underset{D}{(x, y)}}[ (h_{D}(x) - \overline{h}(x))(\overline{h}(x) - y)]
    =
    \mathbb{E}_{(x, y)}
    \mathbb{E}_{D}
    [ (h_{D}(x) - \overline{h}(x))(\overline{h}(x) - y)]
    \\
    =
    \mathbb{E}_{(x, y)}
    [\mathbb{E}_{D}[(h_{D}(x) - \overline{h}(x))]](\overline{h}(x) - y)
    &amp; \\
    =
    \mathbb{E}_{(x, y)}[(\overline{h}(x) - \overline{h}(x))](\overline{h}(x) - y)
    =
    0
  \end{aligned}
\end{equation*}\]</span></p>
<p>Therefore, <span class="math display">\[
\mathbb{E}_{\underset{D \sim P^{n}}{(x, y) \sim P}}[(h_{D}(x) - y)^{2} ]
=
\underbrace{\mathbb{E}_{\underset{D}{x}}[(h_{D}(x) - \overline{h}(x)) ^ {2}]}_{\text{variance of prediction}}
+
\mathbb{E}_{(x, y)}[\underbrace{(\overline{h}(x) - y) ^ {2}}_{\text{average pred - label}}]
\]</span></p>
<p>Putting it all together: <span class="math display">\[
\mathbb{E}_{\underset{D \sim P^{n}}{(x, y) \sim P}}[(h_{D}(x) - y)^{2} ]
=
\underbrace{\mathbb{E}_{\underset{D \sim P^{n}}{x \sim P(x)}}[(h_{D}(x) - \overline{h}(x)) ^ {2}]}_{\text{1}}
+
\underbrace{\mathbb{E}_{(x, y) \sim P}[(\overline{h}(x) - y)^{2}]}_{\text{2}}
\]</span></p>
<p>With <span class="math inline">\(\overline{h}(x)\)</span> is the prediction of the average model and <span class="math inline">\(h_{D}(x)\)</span> the prediction of a specific model.</p>
<p>The two terms can be interpreted as:</p>
<ol class="example" type="1">
<li><p>The difference between the prediction of the average model and a given predictor, i.e.&nbsp;the variance in learning an algorithm given a sampled training data set. This is both due to the training dataset itself and the optimisation process which selects an hypothesis that “appropriately” fits the given training set. Note that this is not a statement about the correctness of a prediction, rather how the predictions from a selected hypothesis will on average vary from the predictions of the “best” (the average) model. This is the <strong><em>variance</em> of the learning algorithm</strong>.</p></li>
<li><p>If we had infinitely many training datasets and from each of those we were to fit a model, we could then make predictions with all those models and the average the predictions. Furthermore if we’re not looking to predict a given <span class="math inline">\(y\)</span> for a given <span class="math inline">\(x\)</span>, but, rather, look to predict the <em>expected label</em>, then we see that the second term is a measures how much error we could still get in this optimal situation<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. This captures how much the fitted model is biased towards some other explanation that is not really in the data. e.g.&nbsp;the data could non-linear (a non-linear decision boundary) and we may be fitting a line to it – thus no matter how much data we have or how many predictors we’re averaging, we will always make some incorrect predictions. This is because the algorithm (the hypothesis space in which we search) is biased towards a specific family of solutions and more data cannot solve this bias. This is what we mean by the <strong>Bias</strong> of the learning algorithm.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;<strong>Optimal</strong> in the sense that now we’ve ammortised the noise in the fitting of the model by taking the average prediction and ammortised the noise in the data by looking to predict the <em>expected label</em></p></li></div><div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>the term is actually the bias squared.</p>
</div>
</div>
</section>
<section id="simulations" class="level3">
<h3 class="anchored" data-anchor-id="simulations">Simulations:</h3>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>ani <span class="op">=</span> animation.FuncAnimation(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    fig, </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    update, </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    interval<span class="op">=</span><span class="dv">300</span>, </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    frames<span class="op">=</span><span class="dv">200</span>, </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    init_func<span class="op">=</span>init, </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    cache_frame_data<span class="op">=</span><span class="va">False</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>HTML(ani.to_jshtml())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>ani <span class="op">=</span> animation.FuncAnimation(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    fig, </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    update, </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    interval<span class="op">=</span><span class="dv">300</span>, </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    frames<span class="op">=</span><span class="dv">200</span>, </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    init_func<span class="op">=</span>init, </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    cache_frame_data<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>HTML(ani.to_jshtml())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="so-what-do-we-expect-for-the-out-of-sample-error" class="level1">
<h1>So what do we expect for the out-of-sample error?</h1>
<section id="classical-error-curve" class="level2">
<h2 class="anchored" data-anchor-id="classical-error-curve">Classical error curve</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="classical_ushape.png" title="Title: Classical U-shape" class="img-fluid figure-img" alt="Classical U-shape." width="3000">
</figure>
<p></p><figcaption class="figure-caption">Classical u-shape</figcaption><p></p>
</figure>
</div>
</section>
<section id="what-i-told-you-is-not-not-wrong-but-not-fully-right-but-it-is-incomplete" class="level2">
<h2 class="anchored" data-anchor-id="what-i-told-you-is-not-not-wrong-but-not-fully-right-but-it-is-incomplete">What I told you is not “Not wrong but not fully right”, but it is incomplete</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="double_descent.png" title="Title: Double descent" class="img-fluid figure-img" alt="Double descent." width="3000">
</figure>
<p></p><figcaption class="figure-caption">Double descent</figcaption><p></p>
</figure>
</div>
<p>It’s been empirically shown that over-parametrised models actually are able to fit the training data exactly and yet generalise exceptionally well.</p>
<p>This was studied in the context of Neural Networks. However it’s actually not the case that only NNs possess such magical properties.</p>
<p>Things that should be considered which we didn’t:</p>
<ul>
<li>Other sources of stochasticity:
<ul>
<li>initialisation</li>
<li>Optimisation, e.g.&nbsp;SGD</li>
<li>Implicit regulariation of the model architecture</li>
<li>Underlying true function</li>
</ul></li>
<li>And very importantly what are good measures of complexity? Are the number of parameters a good measure? Minimum description length and the measures coming from Information Theory, would these be better?
<ul>
<li>e.g.&nbsp;If I have many parameters but contrain them to all have small values (e.g.&nbsp;unit norm of the parameter vectors), is that model as complex as one with, say, half the number of parameters but where the parameter values are unbounded?</li>
</ul></li>
</ul>
<p>So really we need to think about this a little more and venture into more details on the meaning of complexity and the optimisation process</p>

</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-domingos2000unified" class="csl-entry" role="doc-biblioentry">
Domingos, P. 2000. <span>“A Unified Bias-Variance Decomposition.”</span> In <em>Proceedings of 17th International Conference on Machine Learning</em>, 231–38. Morgan Kaufmann Stanford.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>